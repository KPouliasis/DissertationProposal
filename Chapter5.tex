\chapter{The computational side of Jcalc }
\label{jcalccom}
In this section we add proof terms to represent natural deduction constructions. 
The  meaning of these terms emerges naturally from Gentzen's principles that give reduction 
(computational $\beta$-rules) and expansion (extensionality $\eta$-rules) equalities for 
 each construct. We focus on the new constructs of the calculus that emerge from the judgmental interpretation of the $\Box$ connective as explained in 
section \ref{lsjcalc}. In addition, we focus on the \textit{implicational} part. The proof term assignment
for $\wedge$ rules is standard and can be added.

There will be no computational (reduction) rules on  provability terms. 
This conforms with our reading of these terms  as \textit{references} to proof constructs of an \textit{abstracted} theory $J$ that can be realized 
differently for a concrete $J$.  
%This is, we posit, the logical basis for \textit{dynamic linking} under separate compilation. The linker from a language $I$ to any host language $J$ (that satisfies certain specifications) creates residuals dynamically but is not concerned about the actual execution of such residuals. Execution of such residuals happens at a next phase when the references are dereferenced to code of the host and is not observed by the calculus.\footnote{ Such approach, also conforms with justification logic principles. Justifications are static, purely syntactical constructs that are not further analyzed.} 
\section{Proof term assignment}
\label{basicpras}
The following rules and their correspondence with natural deduction  constructs (\ref{jots}) should be obvious to the reader familiar with the simply typed  $\lambda$-calculus and basic justification logic.
We do not repeat  the corresponding $\beta, \eta$ equality rules since they are standard.
\begin{mdframed}[nobreak=true,frametitle={\footnotesize Judgments on Truth  $\Gamma, A,B \in {\sf Prop_0}$  and $M := x_i\  |\  <> \ |\ \lambda x:A.\ M\  |\  (M M) $}]
    \label{jot}
    \mbox{\small
        \begin{mathpar}
            \inferrule*[right=$\Gamma_0$-Refl] {x: A \in \Gamma}{\Turn {\Gamma} { x:A}}
            \and
            \inferrule*[right=$\top_0$I] { }{\Turn {\Gamma} { <>:\top}}
            \and
            \inferrule*[right=$\supset_0$I] {{\Turn {\Gamma, x: A} { M:B}}} {\Turn {\Gamma} { \lambda x:A.\  M:  A\supset  B}}
            \and
            \inferrule*[right=$\supset_0$E] {{\Turn {\Gamma} { M:A\supset  B}}\\{\Turn {\Gamma} {M^{\prime}: A}}} {\Turn {\Gamma} { (MM^{'}):  B}}
            \and
            \inferrule*[right=]{}{+\  \beta\eta \text{\ equalities for \ } \top,\supset}
            %\and
            %\inferrule*[right=$\bot$E] {{\Turn {\Gamma} {\bot}}}{\Turn {\Gamma} {   A}}
        \end{mathpar}}
    \end{mdframed}
    
    For  judgments of ${\sf \llbracket Prop_0\rrbracket}$, we assume a 
    countable set of constant names and demand that every combinatorial
    axiom of intuitionistic logic has  a witness under the interpretation 
    $\llbracket\bullet\rrbracket$. This is what justification logicians call ``axiomatically appropriate constant specification''.
    As usual we demand reflection of contexts in $J$
    and preservation of modus ponens -- closedness under some notion of application (which we denote as $*$).
    
    %\begin{mathpar}
    %\inferrule*[right=$CS$] {{\Turn {\Delta} {\sf \llbracket wf \rrbracket}}\\ {C_i:\llbracket  A\rrbracket \in CS}} {\Delta\vdash_{CS}C_i:\llbracket  A\rrbracket}
    %\end{mathpar}
    
    \begin{mdframed}[nobreak=true,frametitle={\footnotesize Judgments on Validity  $\Delta\in {\sf \llbracket Prop_0\rrbracket}$  and ${\sf J} :=  s_i\ |\ C_i\  | \ {\sf J}*{\sf J} $}]
        \label{justf}
        \mbox{\small
            \begin{mathpar}
                \inferrule*[right=$\Delta$-Refl] {  {s:\llbracket  A\rrbracket \in \Delta}}{\Turn {\Delta} {s:\llbracket  A\rrbracket}}
                \and
                \inferrule*[right=  Ax$_1$]{ }   {\Delta\vdash C_{\top}: \llbracket  \top\rrbracket}
                \and
                \inferrule*[right=  Ax$_2$]{{  A, B \in {\sf Prop_0} }}   {\Delta\vdash C_{K^{A,B}}: \llbracket   A \supset (B \supset   A)\rrbracket }
                \and
                \inferrule*[right=  Ax$_3$]{ {  A,B, C \in {\sf Prop_0}}}{\Delta\vdash C_{S^{A,B,C}}:\llbracket   A\supset (B \supset C) \supset ((  A\supset B) \supset (  A \supset C))\rrbracket}
                \and
                \inferrule*[right=App] {{\Turn {\Delta} { {\sf J}: \llbracket  A \supset  B \rrbracket}}\\ {\Turn{\Delta} {{\sf J'}:\llbracket  A \rrbracket}}}{\Turn {\Delta} { {\sf J*J^\prime}:\llbracket  B\rrbracket}}
                
                
            \end{mathpar}
        }
    \end{mdframed}
    If  $J$ is a proof calculus and $\llbracket\bullet \rrbracket_J$ is  an interpretation such that the specifications above  
    are realized, then $J$ can witness intuitionistic provability. This can be shown by the proof relevant version of the lifting  lemma
    that states:
    \begin{lemma}[$\llbracket\bullet\rrbracket$Lifting Lemma]
        \label{bracklift}
        Given  $\Gamma,  A \in {\sf Prop_0}$ s.t. and a term $M$ s.t. $\Gamma\vdash  M: A $ then there exists ${\sf J}$ s.t  $\llbracket \Gamma\rrbracket \vdash {\sf J}:\llbracket   A \rrbracket$. 
    \end{lemma}
    
    
    
    
    \subsection{Proof term assignment and Gentzen Equalities for $\Box$ Judgments}
    Before we proceed, we will give a small primer of \textit{let}-bindings as used in modern programming languages to provide for some intuition on how such terms work. 
    Let us assume a rudimentary programming language that supports some basic types, say integers (${\sf int}$), as well as pairs of such types. Moreover, let us define a datatype 
    $\sf{Point}$ as a pair of ${\sf int}$ i.e. as $\sf {(int,int)}$ 
    In  a language with \textit{let}-bindings one can define a simple function that takes a ${\sf Point}$ and ``shifts'' it by adding $1$ to each of its $x$ and $y$ coordinates as follows:
    \begin{lstlisting}
    def shift (p:Point) = 
    let  (x,y) be p
    in
    (x+1,y+1)
    \end{lstlisting}
    If we call this function on the point ${\texttt{(2,3)}}$, \
    the computation 
    $$ {\texttt{let (x,y) be (2,3) in (x+1,y+1)}}$$ is invoked. This expression reduces following the \textit{let} reduction rule
    (i.e. pattern matching and substitution) to $\texttt{(2+1,3+1)}$; and as a result we obtain the value $\texttt{(3,4)}$.  As we will see, {\textit{let}} bindings -- with appropriate typing restrictions for our system -- 
    are used in the assignment of proof terms for the $\Box_{IE}$ rule. Moreover, the reduction principle for such terms ($\beta$-rule) -- obtained following Gentzen's equalities for the $\Box$ connective --  
    is exactly the one that we just informally described. 
    
    We can now move forward with the  proof term assignment 
    for the $\Box_{IE}$ rule.  We show first the sub-cases for 
    $\Gamma'$ empty (pure $\Box_I$)  and $\Gamma'$ 
    singleton and explain the computational significance 
    utilizing Gentzen's principles appropriated for the $\Box$ 
    connective. We are  directly translating proof tree equalities 
    from \ref{gprinc} to proof term equalities. 
    We generalize for arbitrary $\Gamma'$ in the following subsection. 
    We have, respectively, the following instances:
    
    \begin{mathpar}
        \inferrule*[]{ {\Gamma\in{\sf Prop_1}}\\{\Turn {\circ} { M:B}}\\{\Turn {\dagger} {{\sf J}:\llbracket  B\rrbracket} }} {\Turn {\Gamma} {  M\& {\sf J}:\Box  B}}
        \and
        \inferrule*[]{{ \Turn {\Gamma}{N:\Box  A}}\\{\Turn {x:A} { M:B}}\\{\Turn {s:\llbracket A \rrbracket} {{\sf J}:\llbracket  B\rrbracket} }} {\Turn {\Gamma} {{\sf let} \ (x\& s \ \ {\sf be\ } N) \ {\sf in}\  (M\& {\sf J}):\Box  B}}
    \end{mathpar}
    
    \subsection{Gentzen's Equalities for  ($\Box$ terms)}
    Gentzen's reduction and expansion principles give computational meaning (dynamics) and an extensionality principle for linking terms. We omit naming the empty contexts for economy.
    
    \mbox{\small
        \begin{mathpar}
            \inferrule*[Right=$I_{\Box B} E_{\Box A}^{x,s}$]{
                {\inferrule*[Left=$\Box_I$]{{\Gamma\in {\sf Prop_1}}\\ \vdash M:A\\ \vdash {\sf j}:\llbracket A \rrbracket}{\Gamma\vdash M \& {\sf j}:\Box A}}\\{x:A\vdash M':B }\\ {s:\llbracket A\rrbracket \vdash {\sf j'}:\llbracket B \rrbracket}}
            {\Gamma\vdash {\sf let} \ \ (x\& s)\ \ {\sf be}\ \   (M\&  {\sf J}) \ \ {\sf in}\ \  { (M^\prime \& {\sf J^\prime})}:\Box B }
        \end{mathpar}
    }$$\Longrightarrow_{R}$$
    \mbox{\small
        \begin{mathpar}
            \inferrule*[Right=$I_{\Box B}$]{\Gamma \in {\sf Prop_1}\\ \vdash M'[M/x]:B \\ \vdash {\sf J^{\prime}}[{\sf J}/s]:\llbracket B \rrbracket}{\Gamma\vdash { M^{\prime}[M/x]\& {\sf J^{\prime}}[{\sf j}/s]}:\Box B} 
        \end{mathpar}
    }
    Where the expressions $M^\prime[M/x]$ and ${\sf J^\prime[J/s]}$ denote capture avoiding substitution, reflecting proof compositionality of the two calculi.
    
    Following the expansion principle we obtain:
    {\small
        $$\begin{array}{c}
        \Gamma\vdash M:\Box   A
        \end{array} \ \Longrightarrow_{E}$$}
    \mbox{\small
        \begin{mathpar}
            \inferrule*[right=$I_{\Box A}E^{{x},{s}}_{\Box A}$]{{ \Turn {\Gamma}{M:\Box  A}}\\{\Turn {x:A} { x:A}}\\{\Turn {s:\llbracket A \rrbracket} {s:\llbracket  A\rrbracket} }} {\Turn {\Gamma} {{\sf let} \ (x\& s \ {\sf be \ } M) \ {\sf in}\  (x\& s):\Box  A}}
        \end{mathpar}}
        
        That gives an $\eta$-equality principle as follows:
        {\small
            $$M:\Box A =_{\eta}\ \ {\sf let} \ \ (x \& s\  {\sf be} \ M)\ \ {\sf in}\ \  { (x \& s)}:\Box A$$
        }
        The $\eta$ equality demands that every $M:\Box A$ should be reducible to a form $M'\& {\sf J^{\prime}}$.  
        \subsection{Proof term assignment for the $\Box$ rule (Generically)}
        After understanding the computational meaning of let expressions in the $\Box_{IE}$ rule 
        we can now give  proof term assignment  for the rule in the general case(i.e. for $\Gamma'$ of arbitrary length). 
        We define a helper syntactic construct --${\sf let}^{*}\ldots {\ \sf in\ }$-- as syntactic sugar for iterative  let bindings based on the structure  of contexts.
        The ${\sf let}^{*}$ macro takes four arguments: a context $\Gamma\in {\sf Prop_0}$, a  context $\Delta\in{\sf \llbracket Prop_1\rrbracket}$,  
        a possibly empty ($[\ ]$) list of terms  $Ns:=N_1,\ldots,  N_i$ - all three of the same length - and a term $M$. It is defined as follows for the empty and non-empty cases:
        %\footnote{Let us stretch here that when we speak about the structure of $\Gamma$ we imply (given the construction of $\Gamma\vdash{\sf wf}$) a treatment of contexts as lists where $\bullet$ stands for the empty list, singleton $x:A$ for $x:A + \bullet$ and contexts written in the form  $\Gamma,x:A$ for $x:A + \Gamma$ where $\Gamma$ is a list. I.e. contexts are lists but written down inversely with the head in the rightmost position. We treat the list of terms $Ns$ similarly for uniformity}  
        
        {\small
            $$\begin{array}{ll}
            \nonumber {\sf let}^{*}\ (\circ;\ \dagger;\  [\ ]) {\ \sf in\ }  M:= M \  &\\
            \nonumber {\sf let}^{*}\ (x_1:A_1,\ldots, x_i:A_i\ ;\  s_1: \phi_1, \ldots, s_i:\phi_i;\  N_1,\ldots,  N_i) {\ \sf in\ } M:= \  & \\
            {\sf let} \ \{(x_1 \& s_1)\  {\sf be}\  N_1,\ldots,  (x_i \& s_i)\  {\sf be}\  N_i\}\ {\sf in}\  M &\\
            \end{array}$$}
        Using this syntactic definition the rule $\Box_{IE}$ rule  can be written compactly:
        
        \begin{mdframed}[nobreak=true,frametitle= \footnotesize{$\Box_{IE}\ \text{ With}\ \Gamma\in {\sf  Prop_1}\text{,}\  \Gamma^{\prime}\in {\sf Prop_0}\text{,{\ \sf length}}(\Gamma)=i\text{,\ }Ns:=N_1 ...\  N_i\text{,}\ 1\le k\le i$} ]% $\Gamma^\prime\in {\sf Prop_0}$ ]
            \mbox{\small
                \begin{mathpar}
                    \inferrule*[right=$I_{\Box B}E^{\vec{x},\vec{s}}_{\Box A_1\ldots \Box A_i}$] %{}
                    {{\forall A_k \in \Gamma^\prime . \   \Gamma \vdash N_k:\Box A_k}\\
                        {\Turn {\Gamma^\prime} {M:B}}\\{\Turn {\llbracket\Gamma^\prime\rrbracket} {{\sf J}:\llbracket B \rrbracket} }} 
                    {\Turn {\Gamma} {{\sf let^{*}}\ (\Gamma^\prime, \llbracket\Gamma^\prime\rrbracket, Ns) \ {\sf in}  \ ( M\& {\sf J}):\Box B}}
                \end{mathpar}
            }
        \end{mdframed}
        It is obvious that all previously mentioned cases are captured with this formulation. The rule of $\beta$-equality can be given  for multi-let bindings directly from Gentzen's reduction principle (\ref{gprinc}) generalized for 
        the multiple intro case: 
        {\small
            $$\begin{array}{ll}
            {\sf let} \{(x_1 \& s_1) {\ \sf be\ } (M_1\& {\sf J_1}),\ldots,  (x_i \& s_i) {\ {\sf be}\ } (M_i\& {\sf J_i})\}\ {\sf in}\  (M \&  {\sf J})& =_{\beta} \\
            {  M[M_1/x_1, \ldots,  M_i/x_i] \& {\sf J}[{\sf J_1}/s_1,\ldots, {\sf J_i}/s_i]}
            \end{array}$$}
        \section{Strong Normalization and small-step semantics}
        In the appendix (\ref{norm}) we provide a proof of normalization for  
        natural deduction (via cut elimination). 
        This is, mutatis mutandis, a strong normalization result for the proof term system too. A  weaker result is 
        normalization under a  deterministic,``call-by-value" reduction strategy for $\beta$-rules.
        This   gives
        an idea of how the system computes  and we can use it in the applications in the next section. 
        As usual we characterize a subset of the closed terms as values and we provide rules for the reduction of the non-value closed terms.
        Note that for the constants of validity and their applicative closure we do not observe reduction properties but treat them as values -- again conforming with the idea of $J$ (and its reduction principles) being ``black boxed''.
        \begin{mdframed}[nobreak=true,frametitle={\footnotesize Small step, call-by-value reduction $\rightarrow$}]
            \begin{mathpar}
                \inferrule*[] { }{\lambda x. M {\ \sf  value}}
                \and
                \inferrule*[] { } {C_i {\ \sf \ value}}
                \and
                \inferrule*[] {{\sf J_1} {\ \sf value} \\ {{\sf J_2} {\ \sf value}} }  {{\sf J_1*J_2} {\ \sf value}}
                \and
                \inferrule*[] {M\  {\sf value} \\ {{\sf J}\  {\sf value}} }  {M \&{\sf J} {\sf \ value}}
                \and
                \inferrule*[] {M \rightarrow M^\prime}  {M \&{\sf J}\rightarrow M^\prime \& {\sf J}}
                \and
                \inferrule*[] {{N_1 \ {\sf value}\  \ldots\text{ \ }  N_{k-1} \ {\sf value}}\\{N_k\rightarrow N_k^{\prime}}}  {{\sf let} \{(x_1 \& s_1) {\ \sf {be}\ } N_1,\ldots,
                    \  (x_{k} \& s_{k}) {\ \sf {be } \ } N_k{\text{,}} \ldots\}    {\ \sf in}\  M \rightarrow \\
                    {\sf let} \{(x_1 \& s_1) {\ \sf {be}\ } N_1,\ldots,
                    \  (x_{k} \& s_{k}) {\ \sf {be } \ } N_k^{\prime}{\text{,}} \ldots\}    {\ \sf in}\  M }
                \and
                \inferrule*[] {M_1 \& {\sf J_1 \ value\ } \ldots\text{\ }  M_i \& {\sf J_i \ value}}  {{\sf let} \{(x_1 \& s_1) {\ \sf be\ } (M_1\& {\sf J_1}),\ldots,  (x_i \& s_i) {\ {\sf be}\ }
                    (M_i\& {\sf J_i})\}\ {\sf in}\  (M \&  {\sf J})\rightarrow \\
                    {  M[M_1/x_1, \ldots,  M_i/x_i] \& {\sf J}[{\sf J_1}/s_1,\ldots, {\sf J_i}/s_i]}}
                \and
                \inferrule*[] {M \rightarrow M^{\prime}}  {(MN) \rightarrow (M^\prime N)}
                \and
                \inferrule*[] {N \rightarrow N^{\prime} }  {((\lambda x. M)N) \rightarrow ((\lambda x. M)N^\prime) }
                \and
                \inferrule*[] {N {\ \sf value}}  {((\lambda x. M)N) \rightarrow [N/x]M }
                %\and
                %\inferrule*[right=$\bot$E] {{\Turn {\Gamma} {\bot}}}{\Turn {\Gamma} {   A}}
            \end{mathpar}
        \end{mdframed}
        Using the reducibility candidates proof method \cite{citeulike:993095}) we show:
        \begin{theorem}[Termination Under Small Step Reduction]
            With $\rightarrow^{*}$ being the reflexive transitive closure of $\rightarrow$: for every closed term $M$ and $A \in {\sf Prop}$ if $\vdash M:A$ then there 
            exists $N \ {\sf value}$ s.t. $\vdash N:A$ and  $M\rightarrow^{*} N$.
        \end{theorem}
        
        \section{A programming language view: Dynamic Linking and separate compilation}
        \label{dlinker}
        Our type system can be related to programming language design when considering \textit{Foreign Function Interfaces}. This is a typical scenario in which a language $I$ interfaces another language $J$ which is  essentially ``black boxed''.
        For example, {\sf OCaml} code  might call {\sf C} code to perform certain computations. 
        In such cases $I$ is a client and $J$ is a host that provides implementations for an interface utilized by the client.
        In the course of software development, the implementations of such an interface might often change (i.e. a new version of the host language, or more dramatically, a complete switch of host language). 
        We want a language design that satisfies
        two  interconnected 
        properties. Firstly, \textit{separate compilation} i.e. when implementations change we do 
        not have to recompile client code and, 
        secondly, \textit{dynamic linking} we want the client code to be linked dynamically to its new 
        ``meaning''.
        
        We will assume that both languages are  functional and based on the lambda calculus. I.e. our interpretation function should have the property $\llbracket A\supset B\rrbracket_J$=
        $\llbracket A\rrbracket_J \llbracket\supset\rrbracket_J \llbracket B\rrbracket_J$ where  $\llbracket\supset\rrbracket_J$ is the implication type constructor in $J$.
        The specifics of the host  $J$ and the concrete implementations are unknown to $I$ but during the linker construction we assume that both languages share  some  basic types
        for otherwise  typed ``communication'' of the two languages would be impossible. 
        Simplifying,  we consider that the only  shared type is  (${\sf int}$), i.e. the linker construction assumes  
        $\bar{n}:\llbracket \sf int \rrbracket$ for every integer $n:{\sf int}$. 
        Let us now assume source code in $I$ that
        is  interfacing   a simple data structure, say an  integer stack\footnote{The details of the stack implementation do not really matter here. It is only ``gluing'' types together correctly that is observed by our type system. Nevertheless, to avoid usage of pair types we assume that the \texttt{pop} operations are ``impure''. I.e. that they modify the very same object and return its top element.},  
        with the following signature ${\sf \Sigma}$:
        \begin{lstlisting} 
        using type intstack
        empty: intstack, 
        push: int -> intstack -> intstack,
        pop: intstack -> int
        \end{lstlisting}
        
        
        And let us consider a simple program in $I$ that is using the signature say, 
        $${\texttt{pop(push (1+1) empty):int}}$$
        This program involves two kinds of computations: a redex $(1+1)$ that can be reduced using the internal semantics of the language $1+1\rightsquigarrow_{I} 2$ and  
        the signature calls ${{\texttt{pop (push 2 empty)}}}$ 
        that are to be performed
        externally  in whichever host language implements them. 
        We treat  dynamic linkers as ``term re-writers'' that map  a computation to its meaning(s) based on different implementations.
        In the following we consider ${\sf \Sigma}$ to be the signature of the interface. Here are the steps towards the linker construction.
        
        \begin{enumerate}
            \item Reduce the source code based on the operational semantics of $I$ until it doesn't have a redex:
            \small{$$ {\sf \Sigma}; \bullet\mathtt{\vdash_{} pop (push \ (1+1) \ Empty)\rightsquigarrow pop (push \ 2 \ Empty) :int}$$}
            \item Contextualize the use of the signature at the final term in step $1$:
            
                \begin{flalign*}
                \mathtt{{\sf \Sigma}; x_1:intstack,  x_2:int\rightarrow intstack\rightarrow intstack, x_3:intstack\rightarrow int} & \\ 
                \mathtt{\vdash x_3 (x_2 \ 2\  x_1):int} &
                \end{flalign*}
            \item Rewrite the previous judgment assuming (abstract) implementations for the corresponding missing elements
            using the ``known'' specification for the shared elements.
                \begin{flalign*}
                \mathtt{ 
                    s_1:\llbracket instack \rrbracket,  s_2:\llbracket int \rightarrow intstack\rightarrow intstack\rrbracket, 
                    s_3:\llbracket intstack\rightarrow int \rrbracket}\\
                \mathtt{\vdash s_3*(s_2* \bar{2}*s_1):\llbracket int\rrbracket}& 
                \end{flalign*}
            \item Combine the two previous judgments using the $\Box_{IE}$ rule.
            {\small
                \begin{flalign*}
                {\sf \Sigma};\mathtt{x_1^{\prime}:\Box intstack ,x_2^{\prime}:\Box(int\rightarrow intstack\rightarrow intstack),} \\
                \mathtt{x_3^{\prime}: \Box  (intstack\rightarrow int)} \\
                \mathtt{\vdash let\{ x_1\& s_1 {\ \sf be \ } x_1^{\prime},\ x_2\& s_2 {\ \sf be \ } x_2^{\prime}, \  x_3\& s_3 {\ \sf be \ } x_3^{\prime} \}\  in}\\  
                \mathtt{(x_3 (x_2\ 2 \ x_1)\ \& \ s_3*(s_2* \bar{2}*s_1))}:\mathtt{\Box int  }
                \end{flalign*}}
            \item Using $\lambda$-abstraction three times we obtain the dynamic linker:
            
                \begin{flalign*}
             {\sf \Sigma};\circ\vdash 
             \mathtt{linker} = \mathtt{\lambda x_1^{\prime}.\  \lambda x_2^{\prime}}. \lambda x_3^{\prime}. 
             \mathtt{let \{ x_1\& s_1 {\ \sf be \ }x_1^{\prime},\ x_2\& s_2 {\ \sf be \ }  x_2^{\prime},\ x_3\& s_3 {\ {\sf be} \ } x_3^{\prime}\}\  in} \\
            \mathtt{(x_3(x_2\ 2 \ x_1)\ \&\  s_3*(s_2*\bar{2}*s_1))} &\\
            \mathtt{:\Box(instack)\rightarrow \Box(int\rightarrow intstack\rightarrow intstack)}\\ 
            \mathtt{\rightarrow \Box (intstack\rightarrow int) \rightarrow \Box int}
                \end{flalign*}
        \end{enumerate}
        Let us see how it can be used in the presence of different implementations:
        \begin{enumerate}
            \item Suppose the developer  responsible for the implementation of the interface is providing an  array based implementation for the stack  in  some language $J$ 
            i.e. we get references to type-checked code fragments of $J$ as follows{\footnote{We have changed the return type of $\mathtt{pop}$ to avoid products. This is just for economy and products can easily be handled.}}:
            {\small
                \begin{flalign*}
                & \mathtt{create():intarray},\  \mathtt{add\_array:int_J \rightarrow_J intarray \rightarrow_J intarray } & \\
                & \mathtt{pop\_array:intarray \rightarrow_J int } &
                \end{flalign*}}
            \item  A unification algorithm check is performed to verify the conformance of the implementations to the signature taking into account 
            fixed type sharing equalities ($\llbracket {\sf int} \rrbracket = {\sf int_J}$). In our case it produces: $$\llbracket\rightarrow\rrbracket = \mathtt{\rightarrow_J}, \llbracket {\sf intstack} \rrbracket= {\sf intarray}$$
            \item
            We thus obtain type-checked links using the $\Box_I$ rule. For example:
            
            \mbox{\small
                \begin{mathpar}
                    \inferrule*[]{{\Turn {{\sf \Sigma};\circ} { \mathtt{push: int \rightarrow intstack \rightarrow intstack }}}\\{\Turn {\bullet} {\mathtt{add\_array:\llbracket int \rightarrow intstack \rightarrow intstack\rrbracket  }} }} {\Turn {{\sf \Sigma};\circ} {  \mathtt{ push\  \& {\ \sf add\_array}:\Box (int \rightarrow intstack \rightarrow intstack)}}}
                \end{mathpar}
            }
            And analogously:
            
            \mbox{\small
                \begin{mathpar}
                    \inferrule*[]{} {{\sf \Sigma}; \circ\vdash {  \mathtt{ pop\  \& \ {\sf pop\_array}:\Box (intstack\rightarrow int)}}}
                    \and
                    \inferrule*[]{} {{\sf \Sigma}; \circ\vdash {  \mathtt{ empty\  \& {\sf create()}:\Box intstack}}}	
                \end{mathpar}
            }
            \item Finally we can compute the next step in the computation for the expression  applying the linker to the obtained pairings:{\small\begin{flalign*}
                {\sf \Sigma}; \bullet\mathtt{\vdash_{}(linker\   (empty\  \& \ create()) \ (push\  \& \ {\sf add\_array}) \ (pop\  \&\  pop\_array))}\\
                \mathtt{:\Box int} 
                \end{flalign*}} which reduces to:
                \begin{flalign*}{\sf \Sigma}; 
                    \bullet\vdash&\mathtt{ let\{ (x_1\& s_1) {\ \sf be \ }(empty\  \& {\sf create()}),\ (x_2\& s_2) {\ \sf be\ } (push\  \& \ add\_array),}\\ 
                &   \mathtt{ (x_3\& s_3) {\ \sf be\ } (pop\  \& \ pop\_array)\}}&\\   
                &\mathtt{in \ (x_3(x_2\ 2 \ x_1) \ \&\  s_3*(s_2*\bar{2}*s_1)):\Box int}&
                \end{flalign*}
            The last expression reduces to ($\beta$-reduction for {\sf let}):{\small\begin{flalign*}
                &{\sf \Sigma}; \bullet\vdash\mathtt{\ pop(push\ 2 \ empty)\ \&\  pop\_array*(add\_array*\bar{2}*empty):\Box int}
                \end{flalign*}}
            giving exactly the next step of the computation for the source expression.
            The good news is that the linker computes correctly the next step given any conforming set of implementations. 
            It is easy to see that given a {\sf list} implementation the very same process would produce a different computation step:{\small\begin{flalign*}
                &{\sf \Sigma}; \bullet\vdash\mathtt{\ pop(push\ 2 \ empty)\ \&\  pop\_list*(Cons*\bar{2}*[]):\Box int}
                \end{flalign*}}
        \end{enumerate}
        We conclude with some remarks that:
        \begin{itemize}
            \item The construction gives a mechanism of abstractions that works not only over different implementations in the
            same language but even for implementations in different (applicative) languages.
            \item We assumed in the example that the  two languages are based on the lambda calculus and implement a curried, higher-order function space. 
            It is easy to see that such host satisfies the requirements for the $\llbracket\bullet\rrbracket$ 
            (with $C_S, C_K$ being the $S, K$ combinators in $\lambda$ form  and $*$ translating to $\lambda$ application).
            \item
            Often, the host language of a foreign call is  not  a language that satisfies  such specifications. This situation occurs  when we have bindings from a functional language to a lower level language \footnote{In this setting the type signature of {\sf push} would be: $\mathtt{\sf int \times intstack\rightarrow instack}$}. 
            Such cases  can be captured by adding conjunction (and pairs), tuning the  specifications of $J$  accordingly and loosening the assumption that $\llbracket \bullet \rrbracket$
            is total on types.
            \item Introduction of  modal types is clearly relative to the $\llbracket\bullet \rrbracket$ function on types. 
            It would be interesting to consider examples where   $\llbracket\bullet \rrbracket$ is realized by non-trivial mappings such as $\llbracket A\supset B \rrbracket= !A \multimap B$
            from the embedding on intuitionistic logic to intuitionistic linear logic.
            That will  showcase an example of   modality that works when lifting to a completely different logic or, correspondingly, to an essentially
            different computational model.
            \item Finally, it should be clear from the operational semantics and this example that we did not demand any equalities (or, reduction rules)  
            for the proofs in $J$, but mere existence of specific terms. This is in accordance to justification logic.  Analogously, we did not observe computation 
            in the host language but only the construction of the linkers as program transformers. We were careful, to say that our calculus corresponds to the dynamic 
            linking part of 
            separate compilation. This, of course, does not tell the whole story of program execution in such cases. Foreign function calls, return the control to the 
            client after the result gets calculated 
            in the external language. For example, the execution of the  program ${\texttt{pop (push 2 empty) + 2}}$ should ``escape'' the client 
            to compute the stack calls and then return
            for the last addition. Our modality captures  exclusively the passing of control from the client to the host 
            dynamically and, as such, is a $K$ 
            (non-factive) modality. Capturing the continuation of the computation and the return of the control back 
            to the source would  require a factive modality and a notion of ``reverse'' of the mapping $\llbracket\bullet\rrbracket$. 
            We touch on this subject in the next chapter and we would like to explore  such an extension in  future work.
            %Here the computation  mixes calling the  external implementation  $(\mathtt{st== empty})$ that -- in a full stack implementation -- would  provide for an equality check, computes its truth value externally and returns the control back to the client language  following the semantics of the $\mathtt{if}$ statement. The full logic for this can  be captured with a stronger modality (i.e. ``factive") that is work in progress. Nevertheless, our work is orthogonal and would correspond to dynamic linking aspect for such a system.
        \end{itemize}
        
        
        
        %\textcolor{green}{\texttt{push  empty\&}}}} $
        
        \section{Related and Future Work}
        \label{relat}
        Directly related work with our calculus, in the same fashion that justification logic and LP \cite{Artemov2001} are related to modal logic, is \cite{Bellin2001}.  The work in \cite{Bellin2001} provides a calculus for explicit assignments (substitutions) which is actually a sub-case of 
        {\sf Jcalc} with $\llbracket\bullet \rrbracket$ being identity. This  sub-case  captures dynamic linking where the host language is the very same one; such need appears in languages with module mechanisms (i.e. implementation hiding and separate compilation within the very same language). In general, the judgmental approach to modality is heavily influenced by \cite{citeulike:5447115}. In a sense, our treatment of validity-as-explicit-provability also generalizes the approach there without having to commit to a ``factive" modality. Finally,  
        important results on programming paradigms related to justification logic have been obtained in \cite{ArtBon07LFCS,BONELLI2012935, bavera2010justification}. 
        Immediate future developments would be to interpret modal formulas of higher degree under the same principles. 
        This corresponds to dynamic linking in two or more steps (i.e., when the host becomes itself a client of another interface that is implemented dynamically in a third level and, so on). 
        Some preliminary results towards this 
        direction have been developed in \cite{DBLP:journals/entcs/PouliasisP14} and we sketch them in the next section. 
        %\nocite{Pfenning2009a, Pfenning2009b}
        

\begin{comment}					
\appendix
        
\label{appen}
\chapter{Appendix}
\subsection{Theorems}
        
        \begin{theorem}[Deduction Theorem for Validity Judgments]
            \label{deduct}
            Given any  $\Gamma,A,B \in {\sf Prop_0}$ then $\Gamma,x:A\vdash B \Longrightarrow \llbracket\Gamma\rrbracket\vdash\llbracket   A\supset B\rrbracket$. 
        \end{theorem}
        \begin{proof}
            The proof proceeds by induction on the derivations $\Gamma,A,B \in {\sf Prop_0}$. Note that the axiomatization of ${\llbracket\sf Prop_0\rrbracket}$ derives the 
            sequents:$\Delta\vdash\llbracket A \supset A\rrbracket$
            for any $\Delta\in {\sf \llbracket Prop_0 \rrbracket}$ (as in combinatory logic the $I$ combinator is derived from $SK$). This handles the reflection case. The rest of the cases are treated exactly as in the proof 
            of completeness of combinatorial axiomatization with respect to the natural deduction in intuitionistic logic. 
            Note that this theorem cannot be proven without the logical specification {\sf $Ax_1$, $Ax_2$}. I.e. it is exactly the requirements of the logical specification that ensure that all  interpretations  
            should be complete with respect to intuitionistic implication.
        \end{proof}
        \begin{lemma}[$\llbracket\bullet\rrbracket$Lifting Lemma]
            \label{bracklift}
            Given  $\Gamma,  A \in {\sf Prop_0}$ then $\Gamma\vdash   A \Longrightarrow \llbracket \Gamma\rrbracket \vdash \llbracket   A \rrbracket$. 
        \end{lemma}
        
        \begin{proof}
            The proof goes by induction on the derivations trivially for all the cases($\supset_{E_0}$ is treated using the ${\sf App}$ rule that internalizes Modus ponens). For the $\supset_{I_0}$ the previous theorem has to be used.
        \end{proof}

        \begin{proof}
            Assuming a derivation $\mathcal {D}$ ::$\Gamma\vdash   A$ from \ref{bracklift} there exists corresponding validity derivation $\mathcal{E}::\llbracket\Gamma\rrbracket\vdash\llbracket   A \rrbracket$. Using the two as premises in the $\Box_{IE}$ with $\Gamma := \Box \Gamma$ we obtain $\Box\Gamma\vdash\Box   A$.
        \end{proof}
        From the previous we get:

        %Let us show an inverse principle to the $\Box$ Lifting Lemma. We define for $A$ in {\sf Prop}:
        %\begin{flalign*}
        %\nonumber \downharpoonright P_i\ & =  P_i \\ \downharpoonright (A_1\supset A_2)&  =  \downharpoonright A_1 \supset \downharpoonright A_2 \\
        %\downharpoonright \Box A & = \downharpoonright A
        %\end{flalign*}
        %And the lifting of the $\downharpoonright$ over $\Gamma\in {\sf Prop}$. We get:
        \begin{theorem}[Collapse $\Box$ Lemma] If $\Box\Gamma\vdash \Box A$ for $\Gamma,A \in {\sf Prop_0}$ then $ \Gamma\vdash  A$.
        \end{theorem}
        \begin{theorem}[Weakening]
            For the N.D. system of {\sf Jcalc}, with $\Gamma, \Gamma^{\prime}\in {\sf Prop_0}$.
            \begin{enumerate}
                \item If  $\Gamma\vdash A$ then $\Gamma,\Gamma^{\prime}\vdash   A$.
                \item If  $\Box\Gamma\vdash \Box   A$ then $\Box\Gamma,\Box\Gamma^{\prime} \vdash \Box A$.
            \end{enumerate}
        \end{theorem} 
        \begin{proof}
            By induction on derivations for the first item. For the second item, given $\Box\Gamma\vdash \Box   A$ by the collapse lemma we get   $\Gamma\vdash   A$ which by the previous item
            gives $\Gamma,\Gamma^{\prime}\vdash   A$.  Using the lifting lemma we get $\llbracket \Gamma,\Gamma^{\prime}\rrbracket \vdash \llbracket  A \rrbracket$.
            Using the last two items we and the $\Box$ rule gives the result.
        \end{proof}
        
        \begin{theorem}[Contraction]
            \begin{enumerate}
                For the N.D. system of Jcalc, with $\Gamma,x:A,B\in {\sf Prop_0}$ 
                \item If  $\Gamma,x:A,x':A,\Gamma^{\prime}\vdash  B$ then $\Gamma,x:A,\Gamma^{\prime}\vdash   B$.
                \item If $\Box\Gamma,x:\Box A,x':\Box A,\Box\Gamma^{\prime}\vdash \Box B$ then $\Box\Gamma,x:\Box A,\Box \Gamma^{\prime}\vdash  \Box B$
            \end{enumerate}  
        \end{theorem}
        \begin{proof}
            Similarly with previous theorem.
        \end{proof}
        \begin{theorem}[Permutation]
            For the N.D. system of Jcalc, with $\Gamma\in {\sf Prop_0}$ and $\pi \Gamma$ the collection of permutations of $\Gamma$.
            \begin{enumerate}
                \item If  $\Gamma\vdash   A$ and $\Gamma^{\prime}\in \pi{\Gamma}$ then $\Gamma'\vdash   A$.
                \item If  $\Box\Gamma\vdash \Box   A$ then  $ \pi\Box\Gamma\vdash \Box   A$.
            \end{enumerate}
        \end{theorem}
        \begin{proof}
            As in the previous item.
        \end{proof}
        \begin{theorem}[Substitution Principle]
            The following hold for both kinds of judgments:
            \begin{enumerate}
                \item If  $\Gamma,x:A\vdash M: B$ and $\Gamma\vdash N: A$ then $\Gamma\vdash M[N/x]: B$ 
                \item If  $\llbracket\Gamma\rrbracket,s:\llbracket A \rrbracket \vdash {\sf J}: \llbracket B\rrbracket$ and 
                $\llbracket\Gamma\rrbracket\vdash {\sf J^{'}}: \llbracket B\rrbracket$ then  $\llbracket\Gamma\rrbracket\vdash  {\sf J[J^{'}/s]}\llbracket B\rrbracket$
            \end{enumerate}
        \end{theorem}
        All previous  theorems can actually be stated for proof terms too. We should discuss the following:
        \begin{theorem}[Deduction Theorem / Emulation of $\lambda$ abstraction]
            \label{deductterms}
            If $\Gamma, A\in {\sf Prop_0}$ and $\Gamma,x:A\vdash M:B$ then there exists ${\sf J}$ s.t.    $\llbracket\Gamma\rrbracket \vdash {\sf J}:\llbracket   A\supset B\rrbracket$.
        \end{theorem}
        \begin{lemma}[$\llbracket\bullet\rrbracket$Lifting Lemma for terms]
            \label{highorder}
            If $\Gamma, A \in {\sf Prop_0}$ and $\Gamma\vdash M: A$ then there exists ${\sf J}$ s.t. $\llbracket \Gamma\rrbracket \vdash {\sf J}:\llbracket   A \rrbracket$. 
        \end{lemma}
        In both theorems the existence of this ${\sf J,J^\prime}$ is algorithmic following the induction proof. 
        \subsection{Linking on the function space}
        The above mentioned algorithms permit  for translating $\lambda$ abstractions to polynomials of $S,K$ combinators which is a standard result in the literature. We do not give the details here but the translation is  syntax driven as it can be seen by the inductive nature of the proofs.
        
        Henceforth, we can generalize the construction in \ref{dlinker} so that it permits for dynamic linking of functions of the client 
        (with missing implementations) such as  $\mathtt{\lambda n:int. push\  n\  empty}$ dynamically given that the host actually implements 
        a higher-order function space (that is it implements the combinators $S,K$ in, say, own lambda calculus $\lambda^{J}$).
        Given implementations of $\mathtt{push\_impl}$, $\mathtt{empty\_impl}$ the linker produces an application expression 
        consisting of $\mathtt{push\_impl}$, $\mathtt{empty\_impl}$, $S$ and $K$.  
        The execution of the target expression will happen in the host after dereferencing  ${\sf push\_impl, empty\_impl}$ (dynamic part) 
        and the combinators $S,K$ (constant part) as, say, lambdas (e.g. $K=\lambda^{J} x.\lambda^{J} y. x$).
        
        \subsection{Gentzen's reduction Principle for $\Box$(General)}
        \label{redmult}
        \mbox{\footnotesize
            \begin{mathpar}
                \inferrule*[right=$  I_{\Box B} E_{\Box   A}^{x,s}$]{
                    \inferrule*{}
                    {\inferrule*[]{}{ \inferrule*[]	{\inferrule*[]{}{\PrTri{$D_1$} \\ \PrTri{$E_1$}}\\\\
                                \inferrule*[]{}{ \ A_1\\ \ \qquad\ \ \  \llbracket   A_1\rrbracket}}{\Box   A_1 }}}\ldots 
                    {\inferrule*[]{}{ \inferrule*[]	{\inferrule*[]{}{\PrTri{$D_i$} \\ \PrTri{$E_1$}}\\\\
                                \inferrule*[]{}{\    A_i\\ \ \qquad \llbracket   A_i\rrbracket}}{\Box   A_i }}}
                    \quad
                    \inferrule*{}
                    {\inferrule*[vdots=1.0em, right=$\vec{x}$]{ }{ A_1\dots A_i}\\\\
                        \inferrule*[]{}{B}} 	   \qquad 
                    \inferrule*{}
                    {\inferrule*[vdots=1.0em, right=$\vec{s}$]{ }{\llbracket   A_1 \ldots A_i \rrbracket}\\\\
                        \inferrule*[]{}{\llbracket B\rrbracket}} 	
                }
                {\Box B} 
            \end{mathpar}
        }
        $$\Longrightarrow_{R}$$
        \mbox{\footnotesize
            \begin{mathpar}
                \inferrule*[right=$I_{\Box B}$]{
                    \inferrule*{}
                    {\inferrule*[vdots=1.0em]{}{\PrTri{$D_1$}\ \PrTri{$D_i$} \\\\ A_1\ldots\ldots  \ \  A_i}\\\\
                        \inferrule*[]{}{B}}
                    \qquad 
                    \inferrule*{}
                    {\inferrule*[vdots=1.0em]
                        {}{\PrTri{$E_1$} \PrTri{$E_i$}\\\\\llbracket   A_1\ldots\ldots A_i\rrbracket}\\\\
                        \inferrule*[]{}{\llbracket B \rrbracket}} 
                }{\Box B}
                
            \end{mathpar}
        }
        \subsection{Notes on the cut elimination proof and normalization of natural deduction}
        \label{norm}
        Standardly, we add the bottom type and elimination rule in the natural deduction and show that in Jcalc + $\bot$: $\centernot\vdash\bot$. The addition goes as follows:
        
        \begin{mathpar}
            \inferrule*[right= Bot] { } {\bot \in {\sf Prop_0}}	
            \and
            \inferrule*[right= $E_\bot$] {{\Gamma\vdash\bot }\\ A\in {\sf Prop}} {\Gamma \vdash A}
        \end{mathpar}
        Our proof strategy follows directly \cite{pfenning2004automated}. We construct an intercalation calculus \cite{sieg1998normal} corresponding to the ${\sf Prop}$ fragment  with the following two judgments:
        \begin{itemize}
            \item[] $A\Uparrow$ for ``Proposition $A$ has normal deduction".
            \item[] $A^\downarrow$ for ``Proposition $A$ is extracted from hypothesis".
        \end{itemize}
        This calculus is, essentially, restricting the natural deduction to canonical derivations. The $\llbracket {\sf judgments} \rrbracket$ are not annotated and are directly ported from the natural deduction since we observe consistency in ${\sf Prop}$. 
        The construction is identical to \cite{pfenning2004automated} (Chapter 3) for the ${\sf Hypotheses},{\sf Coercion},\supset, \bot$ cases, we add the $\Box$ case.
        \begin{mathpar}
            \inferrule*[right=$\Gamma$-hyp]  {x: A\downarrow \in \Gamma^\downarrow}{ \Gamma^\downarrow\vdash^{-} A\downarrow}
            \and
            \inferrule*[right=$\downarrow\Uparrow$] {\Gamma^\downarrow\vdash^{-} A\downarrow}{\Gamma^\downarrow\vdash^{-} A \Uparrow}
            \and
            \inferrule*[right=$\supset$I$^{x}$] {\Gamma^\downarrow, x: A\downarrow\vdash^{-}  B\Uparrow} {\Gamma^\downarrow \vdash^{-}  A\supset  B\Uparrow}
            \inferrule*[right=$\supset$E] {{\Gamma^\downarrow\vdash^{-} A\supset  B \downarrow}\\{\Gamma^\downarrow\vdash^{-}  A\Uparrow}} {\Gamma^\downarrow\vdash^{-}   B\downarrow}
            %\and
            %\inferrule*[right=$\bot$E] {{\Turn {\Gamma} {\bot}}}{\Turn {\Gamma} {   A}}
            \and
            \inferrule*[right= $E_\bot$] {{\Gamma^{\downarrow}\vdash^{-}\bot\downarrow }\\ A\in {\sf Prop}} {\Gamma^{\downarrow}\vdash^{-} A\Uparrow}
            \and
            \inferrule*[right=$\Box_{IE}$ ] {{\Gamma^\downarrow\vdash\Box \Gamma^{\prime}\downarrow}\\{\Gamma'^\downarrow\vdash A\Uparrow }\\ {\llbracket \Gamma^{\prime} \rrbracket\vdash \llbracket A \rrbracket}}{ {\Gamma^\downarrow\vdash \Box A\Uparrow }}
        \end{mathpar}
        Where $\Gamma^\downarrow\vdash\Box \Gamma^{\prime}$ abbreviates $\forall A_i\in \Gamma'. \ \Gamma^{\downarrow}\vdash\Box A_i\downarrow$.
        We prove simultaneously by induction:
        \begin{theorem}[Soundness of Normal Deductions]
            The following hold:
            \begin{enumerate}
                \item If $\Gamma^\downarrow\vdash^{-} A\Uparrow$ then $\Gamma\vdash A$, and
                \item If $\Gamma^\downarrow\vdash^{-} A\downarrow $ then $\Gamma\vdash A$.
            \end{enumerate}
        \end{theorem}
        \begin{proof}
            Simultaneously by induction on derivations.
        \end{proof}
        It is easy to see that this restricted proof system $\centernot\vdash^{-} \bot\Uparrow$. It is hard to show its completeness to the non-restricted natural deduction ($\vdash + \bot_E$ of Jcalc) directly. For that reason we add a rule to make it complete ($\vdash^{+}$) preserving soundness and get a system of Annotated Deductions. We show the correspondence of the restricted system ($\vdash^{-}$) to a cut-free sequent calculus (${\sf JSeq}$), the correspondence of the extended system ($\vdash^{+}$) to ${\sf Jseq + Cut}$ and show cut elimination.\footnote{ In reality, the sequent calculus formulation is built exactly upon intuitions on the intercalation calculus. We refer the reader to the references.}
        
        To obtain completeness we add the rule:
        \begin{mathpar}
            \inferrule*[right=$\Uparrow\downarrow$] {\Gamma^\downarrow\vdash A\Uparrow} {\Gamma^\downarrow\vdash A\downarrow }
        \end{mathpar}
        We define $\vdash^{+} :=\   \ \ \vdash^{-} {\sf with} {\ \sf \Uparrow\downarrow}{\sf Rule}$.
        We show:
        \begin{theorem}[Soundness of Annotated Deductions]
            The following hold:
            \begin{enumerate}
                \item If $\Gamma^\downarrow\vdash^{+} A\Uparrow$ then $\Gamma\vdash A$, and
                \item If $\Gamma^\downarrow\vdash^{+} A\downarrow $ then $\Gamma\vdash A$.
            \end{enumerate}
        \end{theorem}
        \begin{proof}
            As previous item.
        \end{proof}
        
        \begin{theorem}[Completeness of Annotated Deductions]
            \label{compannot}
            The following hold:
            \begin{enumerate}
                \item If $\Gamma\vdash A$ then $\Gamma\downarrow\vdash^{+} A\Uparrow$, and
                \item If $\Gamma\vdash A$ then $\Gamma\downarrow\vdash^{+} A\downarrow$.
            \end{enumerate}
        \end{theorem}
        \begin{proof}
            By induction over the structure of the $\Gamma\vdash A$ derivation.
        \end{proof}
        
        Next we move with devising a sequent calculus formulation corresponding to normal proofs $\Gamma^{\downarrow}\vdash^{-}A\Uparrow$. The calculus that is given in the main body of this theorem. We repeat it here for completeness.
        \begin{mdframed}[nobreak=true,frametitle={\footnotesize Sequent Calculus ($\llbracket {\sf Prop_0} \rrbracket$)}]
            $$\begin{array}{l r}
            \Delta \Rightarrow \llbracket A\rrbracket:= & \exists \Delta'\in \pi(\Delta)\ \text{s.t} \   
            \Delta'\vdash \llbracket A \rrbracket \end{array}$$
            where $\pi(\Delta)$ is the collection of wellformed  $\llbracket {\sf Prop_0} \rrbracket$ contexts $\Delta'\vdash \llbracket {\sf wf}\rrbracket$  with some permutation of the multiset $\Delta$ as co--domain.
        \end{mdframed} 
        
        
        \begin{mdframed}[nobreak=true,frametitle={\footnotesize Sequent Calculus ({\sf Prop})}]
            \mbox{\small
                \begin{mathpar}
                    \inferrule*[right=$Id$] { }{\Gamma, A  \Rightarrow A }
                    
                    \inferrule*[right=$\supset_L$] {{\Gamma, A\supset B, B \Rightarrow  C }\\ {\Gamma, A\supset B \Rightarrow A}} {\Gamma, A\supset B \Rightarrow  C}
                    \and
                    \inferrule*[right=$\supset_R$] {\Gamma, A \Rightarrow  B} {\Gamma \Rightarrow A\supset B}
                    \and
                    \inferrule*[right=$\bot_L$] { } {\Gamma, \bot \Rightarrow A}
                    \and
                    \inferrule*[right=$\Box_{LR}$] {{\Box\Gamma,\Gamma\Rightarrow A}\\{\llbracket\Gamma\rrbracket\Rightarrow \llbracket A \rrbracket }}{\Box\Gamma\Rightarrow \Box A}
                    %\inferrule*[right=$\supset$E] {{\Turn {\Gamma} { A\supset  B}}\\{\Turn {\Gamma} { A}}} {\Turn {\Gamma} {   B}}
                    %\and
                    %\inferrule*[right=$\bot$E] {{\Turn {\Gamma} {\bot}}}{\Turn {\Gamma} {   A}}
                \end{mathpar}}
                %Where the  rule $\Box_{LR}$corresponds to $\Box_{IE}$ and relates the two kinds of sequents 
            \end{mdframed}
            We want to show correspondence of the sequent calculus  w.r.t normal proofs ($\vdash^{-}$).  Two lemmas are required to show soundness. 
            \begin{lemma}[Substitution principle for extractions]
                The following hold:
                \begin{enumerate}
                    \item If $\Gamma_1^\downarrow, x:A^\downarrow,\Gamma_2^\downarrow\vdash^{-} B\Uparrow$ and\\$\Gamma_1^\downarrow\vdash^{-} A\Uparrow$ then  $\Gamma_1^\downarrow,\Gamma_2^\downarrow\vdash^{-} B\Uparrow$
                    \item  If $\Gamma_1^\downarrow, x:A^\downarrow,\Gamma_2^\downarrow\vdash^{-} B\downarrow$ and $\Gamma_1^\downarrow\vdash^{-} A\downarrow$ then $\Gamma_1^\downarrow,\Gamma_2^\downarrow\vdash^{-} B\Uparrow$    
                \end{enumerate}
            \end{lemma}
            \begin{proof}
                Simultaneously by induction on the derivations $A\downarrow$ and $A\Uparrow$.
            \end{proof}
            And making use of the previous we can show, with ($\downharpoonright A$ defined previously):
            \begin{lemma}[Collapse principle for normal deductions]
                The following hold:
                \begin{enumerate}
                    \item If $\Gamma^\downarrow,\vdash^{-}  A\Uparrow$ then $\downharpoonright\Gamma^\downarrow\vdash^{-} \downharpoonright A\Uparrow$   and,
                    \item If $\Gamma^\downarrow\vdash^{-} A\downarrow$ then  $\downharpoonright\Gamma^\downarrow\vdash^{-} \downharpoonright A\downarrow$   
                \end{enumerate}
            \end{lemma}
            Using the previous lemmas and by induction we can show :
            \begin{theorem}[Soundness of the Sequent Calculus] 
                \label{soundnseq}
                If   $\Gamma\Rightarrow B$ then $\Gamma^\downarrow\vdash^{-} B\Uparrow$.
                
                
            \end{theorem}
            \begin{theorem}[Soundness of the Sequent Calculus with Cut] 
                
                If   $\Gamma\Rightarrow^{+} B$ then $\Gamma^\downarrow\vdash^{+} B\Uparrow$.
            \end{theorem}
            
            Next we define the $\Gamma\Rightarrow^{+} A$ as $\Gamma\Rightarrow A$ plus the rule:
            \begin{mathpar}
                \inferrule*[right=Cut]{{\Gamma\Rightarrow^{+} A}\\{\Gamma,A\Rightarrow^{+}B}}{\Gamma\Rightarrow^{+}B}
            \end{mathpar}
            \begin{proof}
                As before. The cut rule case is handled by the $\Uparrow\downarrow$ and substitution for extractions principle showcasing that the correspondence of the cut rule to the coercion from normal to extraction derivations.
            \end{proof}
            Standard structural properties (\textit{Weakening, Contraction}) to show completeness. We do not show these here but they hold.
            \begin{theorem}[Completeness of the Sequent Calculus] 
                \label{compseqcalc}
                The following hold:
                \begin{enumerate}
                    \item If   $\Gamma^\downarrow\vdash^{-} B\Uparrow$ then $\Gamma\Rightarrow B$ and,
                    \item 	If $\Gamma^\downarrow \vdash^{-} A\downarrow$ and $\Gamma,A\Rightarrow B$ then $\Gamma\Rightarrow B$   
                \end{enumerate}
                \begin{proof}
                    Simultaneously by induction on the given derivations making use of the structural properties.
                \end{proof}
                Similarly we show for the extended systems.
                \begin{theorem}[Completeness of the Sequent Calculus with Cut] The following hold:
                    \label{compseqcut}
                    \begin{enumerate}
                        \item If   $\Gamma^\downarrow\vdash^{+} B\Uparrow$ then  $\Gamma\Rightarrow^{+} B$ and,
                        \item 	If $\Gamma^\downarrow \vdash^{+} A\downarrow$ and $\Gamma,A\Rightarrow^{+} B$ then $\Gamma\Rightarrow^{+} B$.   
                    \end{enumerate}
                \end{theorem}
                \begin{proof}
                    As before. The extra case is handled by the Cut rule.
                \end{proof}
            \end{theorem}
            After establishing the correspondence of $\vdash^{-}$ with $\Rightarrow$ and of $\vdash^{+}$ with $\Rightarrow^{+}$ we move on with:
            \begin{theorem}[Admissibility of Cut]
                If $\Gamma\Rightarrow A$ and $\Gamma,A\Rightarrow B$ then $\Gamma\Rightarrow B$.
            \end{theorem}
            The proof is by triple induction on the structure of the formula, and the given derivations and we leave it for a technical report. This gives easily:
            \begin{theorem}[Cut Elimination]
                If $\Gamma\Rightarrow^{+}A$ then $\Gamma\Rightarrow A$.
                
            \end{theorem}
            Which in turn gives us:
            \begin{theorem}[Normalization for Natural Deduction]
                \label{normalization}
                If $\Gamma\vdash A$ then $\Gamma^{\downarrow}\vdash^{-} A\Uparrow$
            \end{theorem}
            \begin{proof}
                From assumption $\Gamma \vdash A$ which by \ref{compannot} gives $\Gamma\vdash^{+} A\Uparrow$. By \ref{compseqcut} and Cut  Elimination we obtain $\Gamma\Rightarrow A$ which by  \ref{soundnseq} completes the proof.
            \end{proof}
            As a result we obtain:

            \begin{proof}
                By contradiction, assume $\vdash\bot$ then $\Rightarrow \bot$ which is not possible.
            \end{proof}
\end{comment}